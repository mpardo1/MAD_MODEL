# Prior distribution
prior = function(param){
a = param[1]
sd = param[2]
aprior = dunif(a, min=0, max=4, log = T)
sdprior = dunif(sd, min=0, max=10, log = T)
return(aprior+sdprior)
}
# Posterior distribution (sum because we work with logarithms)
posterior = function(param, y, forc){
return (likelihood(y,param,forc) + prior(param))
}
proposalfunction = function(param){
vec <- c(rnorm(1, mean = param, sd= 0.5)
,abs(rnorm(1,mean = param[],sd = 0.3)))
return(vec)
}
run_metropolis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations+1,2))
chain[1,] = startvalue
prop_mat <- vector("numeric", length = iterations)
for (i in 1:iterations){
proposal = proposalfunction(chain[i,])
# print("Proposal:")
# print(proposal)
# print(paste0("Iteration:",i))
if(is.na(posterior(proposal,ob_data,forcs_mat))){
print("na likelyhood")
}
probab = exp(posterior(proposal,ob_data,forcs_mat) - posterior(chain[i,],ob_data,forcs_mat))
# print("posterior(proposal,ob_data,forcs_mat):",posterior(proposal,ob_data,forcs_mat))
prop_mat[i] <- probab
if (runif(1) < probab){
chain[i+1,] = proposal
}else{
chain[i+1,] = chain[i,]
}
}
return(chain)
}
startvalue = c(3,0.21)
iterations = 10000
chain = run_metropolis_MCMC(startvalue, iterations)
burnIn = 5000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
par(mfrow = c(2,4))
hist(chain[-(1:burnIn),1],nclass=30, main="Posterior of a", xlab="True value = red line" )
abline(v = mean(chain[-(1:burnIn),1]))
abline(v = true1, col="red" )
hist(chain[-(1:burnIn),2],nclass=30, main="Posterior of b", xlab="True value = red line")
abline(v = mean(chain[-(1:burnIn),2]))
abline(v = true, col="red" )
plot(chain[-(1:burnIn),1], type = "l", xlab="True value = red line" , main = "Chain values of a", )
abline(h = true1, col="red" )
plot(chain[-(1:burnIn),2], type = "l", xlab="True value = red line" , main = "Chain values of b", )
abline(h = trueSD, col="red" )
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
proposal
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
proposal
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
probaba
probab
posterior(proposal,ob_data,forcs_mat)
posterior(chain[i,],ob_data,forcs_mat)
exp(posterior(proposal,ob_data,forcs_mat) - posterior(chain[i,],ob_data,forcs_mat))
posterior(proposal,ob_data,forcs_mat) - posterior(chain[i,],ob_data,forcs_mat)
proposal
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
probab
posterior(proposal,ob_data,forcs_mat)
posterior(chain[i,],ob_data,forcs_mat)
prior(proposal)
dunif(proposal[1], min=0, max=4, log = T)
dunif(proposal[1], min=0, max=4, log = T)
dunif(proposal[1], min=0, max=4, log = T)
proposal[1]
dnorm(-2,sd=4, log = T )
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
posterior(proposal,ob_data,forcs_mat)
posterior(chain[i,],ob_data,forcs_mat)
posterior(proposal,ob_data,forcs_mat) - posterior(chain[i,],ob_data,forcs_mat)
exp(13362180)
exp(133621)
exp(1336)
exp(13)
posterior(proposal,ob_data,forcs_mat)
posterior(chain[i,],ob_data,forcs_mat)
posterior(proposal,ob_data,forcs_mat) - posterior(chain[i,],ob_data,forcs_mat)
exp(-43532)
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
posterior(proposal,ob_data,forcs_mat)
posterior(chain[i,],ob_data,forcs_mat)
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
dnorm(P1, mean = z$P1, sd = sd, log = T)
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
posterior(proposal,ob_data,forcs_mat) - posterior(chain[i,],ob_data,forcs_mat)
proposal
chain[i,]
rnorm(2, mean = 0, sd = 1)
rnorm(2, mean = 0, sd = 1)
rnorm(2, mean = 0, sd = 1)
rnorm(2, mean = 0, sd = 1)
rnorm(2, mean = 0, sd = 1)
rnorm(2, mean = 0, sd = 1)
debugSource('~/MAD_MODEL/SUR_MODEL/Code/test_MH.R')
rm(list = ls())
library("parallel")
library("tidyverse")
library("deSolve")
# Create Pseudo Data:
Path = "~/MAD_MODEL/SUR_MODEL/Code/"
setwd(Path)
system("R CMD SHLIB model_test.c")
dyn.load("model_test.so")
gam1 = 0.2
# We create a vector with the constant parameters.
parms = c(gam1)
# We set the initial conditions to cero.
Y <- c(y1 = 0)
Path = "~/MAD_MODEL/SUR_MODEL/data/Downloads_2378.data"
down <- data.frame(t(read.table(Path, header=FALSE)))
colnames(down) <- c("time", "down")
# List with the data frames of the forcings, sort as the c code.
forcs_mat <- list(data.matrix(down))
min_t <- min(down$time)
max_t <- max(down$time)
times <- seq(min_t,max_t, 1)
out <- ode(Y, times, func = "derivs",
parms = parms, dllname = "model_test",
initfunc = "initmod", nout = 1,
outnames = "Sum", initforc = "forcc",
forcings = down,
fcontrol = list(method = "constant"))
ode <- data.frame(out)
ode$Sum <- NULL
head(ode)
saveRDS(ode, file = "~/MAD_MODEL/SUR_MODEL/Code/ode_pseudo.rds")
# Esta función está adaptada para su uso con el paquete deSolve, el estándar en
# R para las ODE
Path = "~/MAD_MODEL/SUR_MODEL/Code/"
setwd(Path)
system("R CMD SHLIB model_test.c")
dyn.load("model_test.so")
true1 = 0.2
true2 = 0.01
true3 = 1.4
trueSD = 1
# Likelihood function
likelihood <- function(y, #datos
x, # vector con los parámetros
forcings) # forzamientos para el solver de la ode
{
pars <- c(gam1 = x[1]) # death rate group 3
sd <- x[2]
population <- c(y1 = 0.0) #Vector inicial para ODE
forcs_mat <- list(data.matrix(forcings))
z <- ode(y = population,
times = 0:nrow(y), func = "derivs", method = "ode45",
dllname = "model_test", initfunc = "initmod", nout = 0,
parms = pars, initforc = "forcc", forcings = forcs_mat,
fcontrol = list(method = "constant")) #Aquí corre el ODE
colnames(z)[2] <- c("P1")
z <- as.data.frame(z)
z <- z[-1, ]
P1 <- y$X1
res <- #cálculo de la loglikelihood en función de las desviaciones estándar
sum(dnorm(P1, mean = z$P1, sd = sd, log = T))
# print(paste0("res:", res))
return(res)
}
# Registrations:
Path = "~/MAD_MODEL/SUR_MODEL/data/Downloads_2378.data"
down <- data.frame(t(read.table(Path, header=FALSE)))
colnames(down) <- c("time", "down")
head(down)
forcs_mat <- data.matrix(down)
# Pseudo Data to check the oprimization method.
ob_data <- readRDS(file = "~/MAD_MODEL/SUR_MODEL/Code/ode_pseudo.rds")
ob_data <- ob_data[1:300,1:2]
colnames(ob_data) <- c("time", "X1")
l <- nrow(ob_data)
ob_data$X1 <- ob_data$X1 + rnorm(l,0,trueSD)
head(ob_data)
# Example: plot the likelihood profile of the slope.
slopevalues <- function(par){
return(likelihood(ob_data,c(par,trueSD),forcs_mat))
}
vec <- seq(0, 1, by=.01)
slopelikelihoods <- lapply(vec, slopevalues )
plot(vec, slopelikelihoods , type="l", xlab = "values of slope gamma 1", ylab = "Log likelihood")
# Prior distribution
prior = function(param){
a = param[1]
sd = param[2]
aprior = dnorm(a, sd=5,  log = T)
sdprior = dunif(sd, min=0, max=10, log = T)
return(aprior+sdprior)
}
# Posterior distribution (sum because we work with logarithms)
posterior = function(param, y, forc){
return (likelihood(y,param,forc) + prior(param))
}
proposalfunction = function(param){
vec <- param + rnorm(2, mean = 0, sd = 0.1)
return(vec)
}
run_metropolis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations+1,2))
chain[1,] = startvalue
prop_mat <- vector("numeric", length = iterations)
for (i in 1:iterations){
proposal = proposalfunction(chain[i,])
print("Proposal:")
print(proposal)
# print("Proposal:")
# print(proposal)
# print(paste0("Iteration:",i))
if(is.na(posterior(proposal,ob_data,forcs_mat))){
print("na likelyhood")
}
probab = exp(posterior(proposal,ob_data,forcs_mat) - posterior(chain[i,],ob_data,forcs_mat))
# print("posterior(proposal,ob_data,forcs_mat):",posterior(proposal,ob_data,forcs_mat))
print("probab:")
print(probab)
prop_mat[i] <- probab
if (runif(1) < probab){
chain[i+1,] = proposal
}else{
chain[i+1,] = chain[i,]
}
}
return(chain)
}
startvalue = c(3,0.21)
iterations = 10000
chain = run_metropolis_MCMC(startvalue, iterations)
burnIn = 5000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
par(mfrow = c(2,4))
hist(chain[-(1:burnIn),1],nclass=30, main="Posterior of a", xlab="True value = red line" )
abline(v = mean(chain[-(1:burnIn),1]))
abline(v = true1, col="red" )
true1 = 0.2
abline(v = true1, col="red" )
hist(chain[-(1:burnIn),1],nclass=30, main="Posterior of a", xlab="True value = red line" )
abline(v = true1, col="red" )
true1
startvalue = c(0,0.21)
plot(chain[-(1:burnIn),1], type = "l", xlab="True value = red line" , main = "Chain values of a", )
abline(h = true1, col="red" )
((0.001 < .1) * -1.0) * 10^6
((1 < .1) * -1.0) * 10^6
rm(list = ls())
library("parallel")
library("tidyverse")
library("deSolve")
# Create Pseudo Data:
Path = "~/MAD_MODEL/SUR_MODEL/Code/"
setwd(Path)
system("R CMD SHLIB model_test.c")
dyn.load("model_test.so")
gam1 = 0.2
# We create a vector with the constant parameters.
parms = c(gam1)
# We set the initial conditions to cero.
Y <- c(y1 = 0)
Path = "~/MAD_MODEL/SUR_MODEL/data/Downloads_2378.data"
down <- data.frame(t(read.table(Path, header=FALSE)))
colnames(down) <- c("time", "down")
# List with the data frames of the forcings, sort as the c code.
forcs_mat <- list(data.matrix(down))
min_t <- min(down$time)
max_t <- max(down$time)
times <- seq(min_t,max_t, 1)
out <- ode(Y, times, func = "derivs",
parms = parms, dllname = "model_test",
initfunc = "initmod", nout = 1,
outnames = "Sum", initforc = "forcc",
forcings = down,
fcontrol = list(method = "constant"))
ode <- data.frame(out)
ode$Sum <- NULL
head(ode)
saveRDS(ode, file = "~/MAD_MODEL/SUR_MODEL/Code/ode_pseudo.rds")
# Esta función está adaptada para su uso con el paquete deSolve, el estándar en
# R para las ODE
Path = "~/MAD_MODEL/SUR_MODEL/Code/"
setwd(Path)
system("R CMD SHLIB model_test.c")
dyn.load("model_test.so")
ll_ode <- function(x, # vector con los parámetros
forcings, # forzamientos para el solver de la ode
y, # datos
devs){ #desviaciones estándar para calcular la loglikelihood
pars <- c(gam1 = x[1]) # death rate group 3
population <- c(y1 = 0.0) #Vector inicial para ODE
forcs_mat <- list(data.matrix(forcings))
z <- ode(y = population,
times = 0:nrow(y), func = "derivs", method = "ode45",
dllname = "model_test", initfunc = "initmod", nout = 0,
parms = pars, initforc = "forcc", forcings = forcs_mat,
fcontrol = list(method = "constant")) #Aquí corre el ODE
colnames(z)[2] <- c("P1")
z <- as.data.frame(z)
z <- z[-1, ]
P1 <- y$X1
res <- #cálculo de la loglikelihood en función de las desviaciones estándar
sum(dnorm(z$P1 - P1, sd = devs[1], log = T))
if(gam1 < 0){
res <- 0.000001
}
return(res)
}
# Registrations:
Path = "~/MAD_MODEL/SUR_MODEL/data/Observed_data_2300.data"
ob_data <- readRDS(file = "~/MAD_MODEL/SUR_MODEL/Code/ode_pseudo.rds")
colnames(ob_data) <- c("time", "X1")
head(ob_data)
# f_inicio <- as.Date("2021-05-14")
# f_fin <- as.Date("2021-06-12")
#
n_inicio <- min(ob_data$time)
n_fin <- max(ob_data$time)
input2 <- ob_data[n_inicio:n_fin, ]
devs <- c()
spl <- (input2$X1)
fit <- smooth.spline(x = 1:nrow(input2), y = spl, df = 4)
devs[1] <- sd(spl - predict(fit)$y)
# Registrations:
Path = "~/MAD_MODEL/SUR_MODEL/data/Downloads_2378.data"
down <- data.frame(t(read.table(Path, header=FALSE)))
colnames(down) <- c("time", "down")
head(down)
best <- -999999999 #LL inicial a mejorar
# load("seeds_CAN.RData") #Cargamos seeds de los valores iniciales de los pars.
seeds <- matrix( runif(100,0,1), ncol = 100, nrow = 1)
sols <- NA #Pre-aloco el número de combinaciones paramétricas en 2 unidades de LL de la mejor
set.seed(476468713)
condition <- T
round <- 1
# sims <- 2 #Número de combinaciones paramétricas a explorar
sims <- ncol(seeds) #Número de combinaciones paramétricas a explorar
sims <- 1
Cores <- 19 #Numero de cores a utilizar.
it <- 0
while(condition){
#Ahora viene la paralelización
parall <- mclapply(1:sims, mc.cores = Cores, mc.preschedule = F,function(k){
it <- it + 1
fit <- optim(par = seeds[, k], fn = ll_ode, forcings = down, y = input2,
devs = devs,lower=c(0), upper=rep(Inf, 1), control = list(fnscale = -1, maxit = 500, parscale = seeds[, k]))
if((k %% 1000) == 0) {
cat("This was seed no. ", k, "\n")
cat("This fit: ", fit$value, "\n")
}
fit
})
lhs <- parall
rm(parall) #Para evitar fugas de memoria
filename <- paste0("param_MAD_MODEL", round, ".RData") #Salva cada ronda de optimizaciones, por si acaso
save(lhs, file = filename)
# Ahora, recuperamos la loglikelihood de cada combinación de parámetros
logl <- rep(NA, sims)
for(i in 1:sims) logl[i] <- lhs[[i]]$value
# Evaluamos las condiciones para parar el bucle
best2 <- max(logl, na.rm = T)
pc1 <- best < best2
pc2 <- best > (best2 - 2)
sols <- sum(logl > (max(logl, na.rm = T) - 2), na.rm = T)
pc3 <- sols > 1000
condition <- pc1 * pc2 * pc3
condition <- !condition
# Seleccionamos las mejores combinaciones de parámetros para mandar una nueva
# ronda, cogemos las combinaciones que estén a 2 unidades de distancia de la
# mejor, o en su defecto, las 250 mejores combinaciones.
if(sols < 250){
index <- order(logl, decreasing = T)[1:250]
} else {
index <- order(logl, decreasing = T)[1:sols]
}
n <- 1
parmat <- matrix(NA, nrow = length(index), ncol = 1)
for(i in index){
parmat[n, ] <- lhs[[i]]$par
n <- n + 1
}
# Muestreamos las mejores combinaciones de parámetros, cada uno de ellos
# independientemente. De esta forma estamos rompiendo las posibles
# correlaciones entre los parámetros, y en algún sentido, hacemos la
# aproximación menos bayesiana al no samplear la distribución multidimensional
# de los parámetros.
seeds <- t(apply(X = parmat, MARGIN = 2, sample, size = 100000, replace = T))
rm(parmat)
rm(lhs)
print(paste0("Round: ", round, ";  Best likelihood: ", best2,
"; Solutions: ", sols, "\n"))
best <- best2
round <- round + 1
}
cores<-1
Cores <- 1
while(condition){
#Ahora viene la paralelización
parall <- mclapply(1:sims, mc.cores = Cores, mc.preschedule = F,function(k){
it <- it + 1
fit <- optim(par = seeds[, k], fn = ll_ode, forcings = down, y = input2,
devs = devs,lower=c(0), upper=rep(Inf, 1), control = list(fnscale = -1, maxit = 500, parscale = seeds[, k]))
if((k %% 1000) == 0) {
cat("This was seed no. ", k, "\n")
cat("This fit: ", fit$value, "\n")
}
fit
})
lhs <- parall
rm(parall) #Para evitar fugas de memoria
filename <- paste0("param_MAD_MODEL", round, ".RData") #Salva cada ronda de optimizaciones, por si acaso
save(lhs, file = filename)
# Ahora, recuperamos la loglikelihood de cada combinación de parámetros
logl <- rep(NA, sims)
for(i in 1:sims) logl[i] <- lhs[[i]]$value
# Evaluamos las condiciones para parar el bucle
best2 <- max(logl, na.rm = T)
pc1 <- best < best2
pc2 <- best > (best2 - 2)
sols <- sum(logl > (max(logl, na.rm = T) - 2), na.rm = T)
pc3 <- sols > 1000
condition <- pc1 * pc2 * pc3
condition <- !condition
# Seleccionamos las mejores combinaciones de parámetros para mandar una nueva
# ronda, cogemos las combinaciones que estén a 2 unidades de distancia de la
# mejor, o en su defecto, las 250 mejores combinaciones.
if(sols < 250){
index <- order(logl, decreasing = T)[1:250]
} else {
index <- order(logl, decreasing = T)[1:sols]
}
n <- 1
parmat <- matrix(NA, nrow = length(index), ncol = 1)
for(i in index){
parmat[n, ] <- lhs[[i]]$par
n <- n + 1
}
# Muestreamos las mejores combinaciones de parámetros, cada uno de ellos
# independientemente. De esta forma estamos rompiendo las posibles
# correlaciones entre los parámetros, y en algún sentido, hacemos la
# aproximación menos bayesiana al no samplear la distribución multidimensional
# de los parámetros.
seeds <- t(apply(X = parmat, MARGIN = 2, sample, size = 100000, replace = T))
rm(parmat)
rm(lhs)
print(paste0("Round: ", round, ";  Best likelihood: ", best2,
"; Solutions: ", sols, "\n"))
best <- best2
round <- round + 1
}
while(condition){
#Ahora viene la paralelización
parall <- mclapply(1:sims, mc.cores = Cores, mc.preschedule = F,function(k){
it <- it + 1
fit <- optim(par = seeds[, k], fn = ll_ode, forcings = down, y = input2,
devs = devs, control = list(fnscale = -1, maxit = 500, parscale = seeds[, k]))
if((k %% 1000) == 0) {
cat("This was seed no. ", k, "\n")
cat("This fit: ", fit$value, "\n")
}
fit
})
lhs <- parall
rm(parall) #Para evitar fugas de memoria
filename <- paste0("param_MAD_MODEL", round, ".RData") #Salva cada ronda de optimizaciones, por si acaso
save(lhs, file = filename)
# Ahora, recuperamos la loglikelihood de cada combinación de parámetros
logl <- rep(NA, sims)
for(i in 1:sims) logl[i] <- lhs[[i]]$value
# Evaluamos las condiciones para parar el bucle
best2 <- max(logl, na.rm = T)
pc1 <- best < best2
pc2 <- best > (best2 - 2)
sols <- sum(logl > (max(logl, na.rm = T) - 2), na.rm = T)
pc3 <- sols > 1000
condition <- pc1 * pc2 * pc3
condition <- !condition
# Seleccionamos las mejores combinaciones de parámetros para mandar una nueva
# ronda, cogemos las combinaciones que estén a 2 unidades de distancia de la
# mejor, o en su defecto, las 250 mejores combinaciones.
if(sols < 250){
index <- order(logl, decreasing = T)[1:250]
} else {
index <- order(logl, decreasing = T)[1:sols]
}
n <- 1
parmat <- matrix(NA, nrow = length(index), ncol = 1)
for(i in index){
parmat[n, ] <- lhs[[i]]$par
n <- n + 1
}
# Muestreamos las mejores combinaciones de parámetros, cada uno de ellos
# independientemente. De esta forma estamos rompiendo las posibles
# correlaciones entre los parámetros, y en algún sentido, hacemos la
# aproximación menos bayesiana al no samplear la distribución multidimensional
# de los parámetros.
seeds <- t(apply(X = parmat, MARGIN = 2, sample, size = 100000, replace = T))
rm(parmat)
rm(lhs)
print(paste0("Round: ", round, ";  Best likelihood: ", best2,
"; Solutions: ", sols, "\n"))
best <- best2
round <- round + 1
}
